---
---

@string{aps = {American Physical Society,}}

@article{gao2025smartrag,
  abbr={smartrag},
  title={Smart{RAG}: Jointly Learn {RAG}-Related Tasks From the Environment Feedback}, 
  author={Jingsheng Gao and Linxu Li and Ke Ji and Weiyuan Li and Yixin Lian and yuzhuo fu and Bin Dai},
  abstract={RAG systems consist of multiple modules to work together. However, these modules are usually separately trained. We argue that a system like RAG that incorporates multiple modules should be jointly optimized to achieve optimal performance. To demonstrate this, we design a specific pipeline called SmartRAG that includes a policy network and a retriever. The policy network can serve as 1) a decision maker that decides when to retrieve, 2) a query rewriter to generate a query most suited to the retriever and 3) an answer generator that produces the final response with/without the observations. We then propose to jointly optimize the whole system using a reinforcement learning algorithm, with the reward designed to encourage the system to achieve the highest performance with minimal retrieval cost. When jointly optimized, each module can be aware of how other modules are working and thus find the best way to work together as a complete system. Empirical results demonstrate that the jointly optimized system can achieve better performance than separately optimized counterparts.},
  journal={The Thirteenth International Conference on Learning Representations (ICLR)},
  year={2025},
  url={https://openreview.net/forum?id=OCd3cffulp},
  html={https://openreview.net/forum?id=OCd3cffulp},
  dimensions={true},
  selected={true}
}

@article{han2023dialcot,
  abbr={dialcot},
  title={DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models}, 
  author={Chengcheng Han and Xiaowei Du and Che Zhang and Yixin Lian and Xiang Li and Ming Gao and Baoyuan Wang},
  abstract={Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective or even detrimental when applied to reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters. To address this limitation, we introduce Dialogue-guided Chain-of-Thought (DialCoT) which employs a dialogue format to generate intermediate reasoning steps, guiding the model toward the final answer. Additionally, we optimize the model's reasoning path selection using the Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning capabilities. Our method offers several advantages compared to previous approaches. Firstly, we transform the process of solving complex reasoning questions by breaking them down into a series of simpler sub-questions, significantly reducing the task difficulty and making it more suitable for SLMs. Secondly, we optimize the model's reasoning path selection through the PPO algorithm. We conduct comprehensive experiments on four arithmetic reasoning datasets, demonstrating that our method achieves significant performance improvements compared to state-of-the-art competitors.},
  journal={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023},
  url={https://arxiv.org/abs/2310.05074},
  html={https://arxiv.org/abs/2310.05074},
  dimensions={true},
  selected={true}
}

@article{ji2023hierarchical,
  abbr={hierarchical},
  title={Hierarchical Verbalizer for Few-Shot Hierarchical Text Classification},
  author={Ke Ji and Yixin Lian and Jingsheng Gao and Baoyuan Wang},
  abstract={We propose the hierarchical verbalizer ("HierVerb"), a multi-verbalizer framework treating HTC as a single- or multi-label classification problem at multiple layers and learning vectors as verbalizers constrained by hierarchical structure and hierarchical contrastive learning.},
  journal={Conference of the Association for Computational Linguistics (ACL)},
  year={2023},
  url={https://arxiv.org/abs/2305.16885},
  html={https://arxiv.org/abs/2305.16885},
  dimensions={true},
  selected={true}
}

@article{gao2023livechat,
  abbr={livechat},
  title={LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming},
  author={Jingsheng Gao and Yixin Lian and Ziyi Zhou and Yuzhuo Fu and Baoyuan Wang},
  abstract={To improve the essential capability of responding and establish a benchmark in the live open-domain scenario, we introduce the LiveChat dataset, composed of 1.33 million real-life Chinese dialogues with almost 3800 average sessions across 351 personas and fine-grained profiles for each persona. LiveChat is automatically constructed by processing numerous live videos on the Internet and naturally falls within the scope of multi-party conversations, where the issues of Who says What to Whom should be considered. },
  journal={Conference of the Association for Computational Linguistics (ACL)},
  year={2023},
  url={https://arxiv.org/abs/2306.08401},
  html={https://arxiv.org/abs/2306.08401},
  dimensions={true},
  selected={true}
}

@article{2022patentDialogSys,
  abbr={dialoguesys},
  title={一种对话模型生成、应用方法、系统、设备及存储介质},
  author={连怡鑫 and 刘剑锋 and 杜晓薇 and 王宝元},
  abstract={本发明公开了一种对话模型生成、应用方法、系统、设备及存储介质，涉及计算机技术领域。一种对话模型生成方法通过获取预设的对话样本；对对话样本进行预处理，得到三元组数据；将三元组数据输入预设的自然语言模型进行语义理解，输出与三元组数据对应的语义意图；将三元组数据和语义意图输入初始神经网络进行训练，得到对话模型。再通过一种对话模型应用方法应用对话模型生成方法生成的对话模型，实现通过多源多模态的数据训练出对话效果较好的对话模型，并基于应用对话模型，**地提高了对话模型的对话迁移场景能力，增强对话回复的信息丰富度和准确率。},
  journal={发明专利:	CN115438170A},
  year={2022},
  url={https://aiqicha.baidu.com/patent/info?referId=a86ef7bea7d15e400bfca2a5c8a42c9207b2afac&pid=31720823858720},
  html={https://aiqicha.baidu.com/patent/info?referId=a86ef7bea7d15e400bfca2a5c8a42c9207b2afac&pid=31720823858720},
  dimensions={true},
  selected={true}
}

@article{2022patentLiveChat,
  abbr={patentlivechat},
  title={基于直播场景的对话数据集构建方法及装置},
  author={高景盛 and 连怡鑫 and 王宝元},
  abstract={本发明实施例提供一种基于直播场景的对话数据集构建方法及装置，该方法包括：基于主播直播视频的用户评论，获取用户评论发表后预设时间内文本格式的主播讲话内容；获取主播讲话内容中与用户评论具有重叠词汇的句子并计算和用户评论间的语义相似度，根据语义相似度获取针对用户评论的主播回复；根据用户评论和主播回复构建对话数据集。本发明实施例基于真实直播场景下大量的主播直播视频和用户评论，通过时间匹配、词汇匹配及相似度判断构建对话数据集，得到了特定人物特征下的大规模真实数据集，有利于训练得到具有丰富人物特征且在泛化性、多样性、相关性方面表现良好的对话系统，有利于真实场景对话系统的效果提升，提升用户体验度。},
  journal={发明专利:	CN115544237A},
  year={2022},
  url={https://aiqicha.baidu.com/patent/info?referId=9495008b8c4b8a01dc316bcfdbdcdf79677dc79d&pid=31720823858720},
  html={https://aiqicha.baidu.com/patent/info?referId=9495008b8c4b8a01dc316bcfdbdcdf79677dc79d&pid=31720823858720},
  dimensions={true},
  selected={true}
}

@article{Guo2021DetectingLA,
  abbr={detectingla},
  title={Detecting Log Anomalies with Multi-Head Attention (LAMA)},
  author={Yicheng Guo and Yujin Wen and Congwei Jiang and Yixin Lian and Yi Wan},
  abstract={We propose LAMA, a multi-head attention based sequential model to process log streams as template activity (event) sequences.},
  journal={Arxiv},
  year={2021},
  url={https://arxiv.org/abs/2101.02392},
  html={https://arxiv.org/abs/2101.02392},
  dimensions={true},
  selected={true}
}


@article{Han2020MultiWOZ2A,
  abbr={multiwoz2a},
  title={MultiWOZ 2.3: A Multi-domain Task-Oriented Dialogue Dataset Enhanced with Annotation Corrections and Co-Reference Annotation},
  author={Ting Han and Ximing Liu and Ryuichi Takanobu and Yixin Lian and Chongxuan Huang and Dazhen Wan and Wei Peng and Minlie Huang},
  abstract={ In this paper, we introduce MultiWOZ 2.3, in which we differentiate incorrect annotations in dialogue acts from dialogue states, identifying a lack of co-reference when publishing the updated dataset.},
  journal={Natural Language Processing and Chinese Computing (NLPCC)},
  year={2020},
  url={https://arxiv.org/abs/2010.05594},
  html={https://arxiv.org/abs/2010.05594},
  dimensions={true},
  selected={true}
}

